{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cdffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import string \n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bc6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/'\n",
    "web = requests.get(url).text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddbf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48eae43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2ced26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fcbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in soup(['script', 'style']):\n",
    "    s.decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6c04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(soup.stripped_strings)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ec93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = text\n",
    "# full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60096fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5694da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881a3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14631\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c0efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'if', 'the', 'creation', 'is', 'taking', 'over', 'the', 'creator', 'blackcoffer', 'insights', 'sign', 'in', 'our', 'success', 'stories', 'banking', 'financials', 'securities', 'and', 'insurance', 'energy', 'entertainment', 'fast', 'moving', 'consumer', 'goods', 'government', 'think', 'tanks', 'healthcare', 'infrastructure', 'real', 'estate', 'it', 'lifestyle', 'ecommerce', 'online', 'market', 'place', 'production', 'manufacturing', 'research', 'academia', 'retail', 'supply', 'chain', 'telecom', 'what', 'we', 'do', 'banking', 'financials', 'securities', 'and', 'insurance', 'energy', 'entertainment', 'fast', 'moving', 'consumer', 'goods', 'government', 'think', 'tanks', 'healthcare', 'hospitality', 'infrastructure', 'real', 'estate', 'it', 'services', 'lifestyle', 'ecommerce', 'online', 'market', 'place', 'news', 'media', 'production', 'manufacturing', 'research', 'academia', 'retail', 'supply', 'chain', 'what', 'we', 'think', 'automobiles', 'components', 'bfsi', 'asset', 'and', 'portfolio', 'banks', 'capital', 'markets', 'derivatives', 'and', 'securities', 'diversified', 'financials', 'finance', 'accounting', 'insurance', 'securities', 'and', 'capital', 'markets', 'capital', 'goods', 'commercial', 'professional', 'services', 'consumer', 'discretionary', 'consumer', 'durables', 'apparel', 'consumer', 'services', 'consumer', 'staples', 'food', 'staples', 'retailing', 'food', 'beverage', 'tobacco', 'household', 'personal', 'products', 'data', 'science', 'analytics', 'artificial', 'intelligence', 'big', 'data', 'business', 'analytics', 'data', 'visualization', 'internet', 'of', 'things', 'machine', 'learning', 'statistics', 'energy', 'dataoil', 'how', 'to', 'analytics', 'application', 'development', 'artificial', 'intelligence', 'business', 'analytics', 'example', 'optimization', 'projects', 'software', 'development', 'source', 'code', 'audit', 'statistics', 'web', 'mobile', 'app', 'development', 'schedule', 'demo', 'contact', 'sign', 'in', 'welcome', 'log', 'into', 'your', 'account', 'your', 'username', 'your', 'password', 'forgot', 'your', 'password', 'password', 'recovery', 'recover', 'your', 'password', 'your', 'email', 'search', 'sign', 'in', 'welcome', 'log', 'into', 'your', 'account', 'your', 'username', 'your', 'password', 'forgot', 'your', 'password', 'get', 'help', 'password', 'recovery', 'recover', 'your', 'password', 'your', 'email', 'a', 'password', 'will', 'be', 'emailed', 'to', 'you', 'monday', 'april', 'sign', 'in', 'join', 'our', 'success', 'stories', 'what', 'we', 'do', 'what', 'we', 'think', 'how', 'to', 'schedule', 'demo', 'contact', 'facebook', 'linkedin', 'twitter', 'youtube', 'our', 'success', 'stories', 'all', 'banking', 'financials', 'securities', 'and', 'insurance', 'energy', 'entertainment', 'fast', 'moving', 'consumer', 'goods', 'government', 'think', 'tanks', 'healthcare', 'infrastructure', 'real', 'estate', 'it', 'lifestyle', 'ecommerce', 'online', 'market', 'place', 'production', 'manufacturing', 'our', 'success', 'stories', 'ranking', 'customer', 'behaviours', 'for', 'business', 'strategy', 'december', 'our', 'success', 'stories', 'algorithmic', 'trading', 'for', 'multiple', 'commodities', 'markets', 'like', 'forex', 'metals', 'energy', 'etc', 'december', 'our', 'success', 'stories', 'trading', 'bot', 'for', 'forex', 'december', 'our', 'success', 'stories', 'python', 'model', 'for', 'the', 'analysis', 'of', 'sectorspecific', 'stock', 'etfs', 'for', 'investment', 'purposes', 'december', 'what', 'we', 'do', 'all', 'banking', 'financials', 'securities', 'and', 'insurance', 'energy', 'entertainment', 'fast', 'moving', 'consumer', 'goods', 'government', 'think', 'tanks', 'healthcare', 'hospitality', 'infrastructure', 'real', 'estate', 'it', 'services', 'lifestyle', 'ecommerce', 'online', 'market', 'place', 'what', 'we', 'do', 'playstore', 'appstore', 'to', 'google', 'analytics', 'ga', 'or', 'firebase', 'to', 'google', 'data', 'studio', 'mobile', 'app', 'kpi', 'dashboard', 'september', 'what', 'we', 'do', 'google', 'local', 'service', 'ads', 'lsa', 'api', 'to', 'google', 'bigquery', 'to', 'google', 'data', 'studio', 'may', 'our', 'success', 'stories', 'ai', 'conversational', 'bot', 'using', 'rasa', 'february', 'what', 'we', 'do', 'recommendation', 'system', 'architecture', 'october', 'what', 'we', 'think', 'all', 'automobiles', 'components', 'bfsi', 'asset', 'and', 'portfolio', 'banks', 'capital', 'markets', 'derivatives', 'and', 'securities', 'diversified', 'financials', 'finance', 'accounting', 'insurance', 'securities', 'and', 'capital', 'markets', 'what', 'we', 'think', 'rise', 'of', 'telemedicine', 'and', 'its', 'impact', 'on', 'livelihood', 'by', 'the', 'year', 'january', 'what', 'we', 'think', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'what', 'we', 'think', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'what', 'we', 'think', 'rise', 'of', 'chatbots', 'and', 'its', 'impact', 'on', 'customer', 'support', 'by', 'the', 'year', 'january', 'how', 'to', 'all', 'analytics', 'application', 'development', 'artificial', 'intelligence', 'business', 'analytics', 'example', 'optimization', 'projects', 'software', 'development', 'source', 'code', 'audit', 'statistics', 'what', 'we', 'do', 'aiml', 'and', 'predictive', 'modeling', 'february', 'blackcoffer', 'solution', 'for', 'contact', 'centre', 'problems', 'april', 'how', 'to', 'how', 'to', 'setup', 'custom', 'domain', 'for', 'google', 'app', 'engine', 'application', 'february', 'how', 'to', 'code', 'review', 'checklist', 'april', 'schedule', 'demo', 'contact', 'search', 'home', 'what', 'we', 'think', 'what', 'if', 'the', 'creation', 'is', 'taking', 'over', 'the', 'creator', 'data', 'science', 'artificial', 'intelligence', 'big', 'data', 'business', 'analytics', 'machine', 'learning', 'what', 'we', 'think', 'what', 'if', 'the', 'creation', 'is', 'taking', 'over', 'the', 'creator', 'by', 'ajay', 'bidyarthy', 'june', 'facebook', 'twitter', 'pinterest', 'whatsapp', 'human', 'minds', 'a', 'fascination', 'in', 'itself', 'carrying', 'the', 'potential', 'of', 'tinkering', 'nature', 'with', 'the', 'pixie', 'dust', 'intelligence', 'creating', 'and', 'solving', 'the', 'mysteries', 'and', 'wonders', 'with', 'anything', 'but', 'admiration', 'however', 'no', 'matter', 'how', 'captivating', 'a', 'human', 'mind', 'can', 'be', 'it', 'could', 'sometimes', 'be', 'appalled', 'it', 'could', 'be', 'the', 'hunger', 'or', 'maybe', 'the', 'desire', 'to', 'want', 'more', 'to', 'go', 'beyond', 'and', 'unravel', 'the', 'limitations', 'or', 'maybe', 'something', 'like', 'pure', 'greed', 'humans', 'have', 'never', 'stopped', 'and', 'always', 'keep', 'evolving', 'when', 'it', 'comes', 'to', 'intelligence', 'and', 'this', 'is', 'what', 'makes', 'them', 'the', 'supreme', 'intelligence', 'calls', 'out', 'for', 'supremacy', 'and', 'so', 'what', 'if', 'there', 'was', 'to', 'evolve', 'something', 'that', 'opposed', 'a', 'challenge', 'to', 'the', 'very', 'human', 'minds', 'to', 'their', 'capabilities', 'while', 'making', 'them', 'question', 'their', 'own', 'importance', 'among', 'themselves', 'artificial', 'intelligence', 'came', 'as', 'a', 'revolution', 'havoc', 'when', 'it', 'first', 'came', 'to', 'the', 'light', 'the', 'concept', 'of', 'making', 'machines', 'does', 'work', 'on', 'their', 'own', 'like', 'granting', 'machines', '–the', 'intelligence', 'the', 'idea', 'of', 'making', 'machines', 'work', 'like', 'humans', 'came', 'back', 'in', 'the', 's', 'back', 'then', 'people', 'didn’t', 'believe', 'in', 'such', 'a', 'thing', 'as', 'making', 'a', 'nonliving', 'thing', 'work', 'think', 'and', 'carry', 'tasks', 'on', 'its', 'own', 'not', 'to', 'mention', 'to', 'actually', 'surpass', 'humans', 'themselves', 'in', 'those', 'skills', 'the', 'facts', 'are', 'it', 'did', 'by', 'the', 'greatest', 'chess', 'player', 'garry', 'kasparov', 'was', 'defeated', 'in', 'a', 'chess', 'game', 'by', 'a', 'machine', 'and', 'this', 'is', 'where', 'exactly', 'a', 'top', 'skilled', 'human', 'lost', 'to', 'a', 'mere', 'machine', 'created', 'by', 'another', 'who', 'by', 'himself', 'could’ve', 'never', 'defeated', 'him', 'it', 'was', 'a', 'rule', 'of', 'power', 'of', 'betterment', 'of', 'skills', 'and', 'the', 'granted', 'supremacy', 'were', 'ai', 'and', 'machines', 'just', 'tools', 'equipment', 'something', 'that', 'helped', 'an', 'unskilled', 'person', 'with', 'his', 'mind', 'and', 'intelligence', 'creates', 'something', 'that', 'could', 'do', 'the', 'skilled', 'work', 'for', 'him', 'with', 'perfection', 'and', 'precision', 'well', 'initially', 'it', 'was', 'however', 'as', 'time', 'passed', 'as', 'humans', 'got', 'drawn', 'to', 'the', 'puzzle', 'of', 'ai', 'a', 'lot', 'changed', 'human', 'research', 'went', 'deeper', 'and', 'deeper', 'and', 'as', 'a', 'result', 'the', 'machines', 'evolved', 'with', 'it', 'at', 'present', 'ai', 'machines', 'is', 'a', 'growing', 'field', 'as', 'it', 'develops', 'and', 'improves', 'it', 'has', 'become', 'a', 'part', 'of', 'the', 'industrial', 'revolution', 'in', 'industries', 'most', 'of', 'the', 'laborious', 'work', 'that', 'was', 'once', 'taken', 'care', 'of', 'by', 'humans', 'was', 'now', 'replaced', 'by', 'machines', 'naturally', 'with', 'the', 'evolution', 'in', 'machines', 'its', 'precision', 'mass', 'productivity', 'quality', 'control', 'time', 'efficiencies', 'and', 'all', 'the', 'other', 'factors', 'made', 'it', 'a', 'better', 'choice', 'a', 'choice', 'over', 'humans', 'this', 'led', 'to', 'fear', 'a', 'fear', 'of', 'a', 'notsodistant', 'future', 'a', 'future', 'where', 'maybe', 'machines', 'will', 'be', 'so', 'evolved', 'that', 'they’ll', 'take', 'over', 'the', 'need', 'of', 'a', 'human', 'employee', 'leading', 'to', 'unemployment', 'with', 'the', 'population', 'increase', 'around', 'the', 'world', 'it', 'became', 'the', 'new', 'tech', 'threat', 'for', 'the', 'labor', 'market', 'then', 'again…', 'how', 'true', 'is', 'it', 'does', 'ai', 'really', 'oppose', 'a', 'threat', 'will', 'adapting', 'to', 'technology', 'make', 'millions', 'of', 'people', 'lose', 'their', 'jobs', 'will', 'it', 'lead', 'to', 'mass', 'unemployment', 'will', 'the', 'machines', 'really', 'surpass', 'humans', 'will', 'the', 'creation', 'take', 'over', 'the', 'creator', 'no', 'matter', 'how', 'fearful', 'the', 'future', 'with', 'ai', 'may', 'seem', 'in', 'reality', 'it', 'is', 'not', 'that', 'scary', 'truth', 'is', 'ai', 'is', 'the', 'present', 'reality', 'it', 'is', 'the', 'key', 'that', 'holds', 'the', 'power', 'to', 'unlock', 'a', 'whole', 'next', 'level', 'of', 'human', 'evolution', 'technology', 'is', 'growing', 'there', 'was', 'a', 'time', 'where', 'technology', 'was', 'just', 'an', 'idea', 'but', 'today', 'that', 'idea', 'has', 'been', 'implemented', 'it’s', 'working', 'and', 'is', 'carried', 'out', 'nobody', 'could', 'stop', 'the', 'advancement', 'and', 'growth', 'of', 'artificial', 'intelligence', 'it’s', 'a', 'wave', 'that', 'is', 'already', 'flowing', 'and', 'we', 'as', 'the', 'present', 'generation', 'and', 'the', 'generations', 'to', 'come', 'to', 'have', 'to', 'learn', 'to', 'learn', 'to', 'swim', 'in', 'this', 'flow', 'and', 'avoid', 'drowning', 'many', 'jobs', 'will', 'be', 'replaced', 'by', 'machines', 'as', 'ai', 'evolves', 'it’ll', 'keep', 'challenging', 'human', 'minds', 'and', 'their', 'skills', 'with', 'the', 'present', 'covid', 'situation', 'contactless', 'cashiers', 'to', 'robots', 'delivering', 'packages', 'have', 'already', 'taken', 'over', 'the', 'usual', 'routine', 'tasks', 'the', 'jobs', 'of', 'secretaries', 'schedulers', 'and', 'bookkeeper', 'are', 'at', 'risk', 'too', 'manufacturing', 'units', 'agriculture', 'food', 'services', 'retail', 'transportation', 'logistic', 'and', 'hospitality', 'are', 'all', 'a', 'part', 'of', 'the', 'aiaffected', 'automation', 'at', 'an', 'estimation', 'it', 'is', 'said', 'that', 'around', 'million', 'jobs', 'especially', 'including', 'manufacturing', 'will', 'be', 'lost', 'to', 'robots', 'as', 'ai', 'robotics', 'd', 'printing', 'and', 'genetics', 'make', 'their', 'way', 'in', 'even', 'the', 'architects', 'medical', 'docs', 'and', 'music', 'composers', 'feel', 'threatened', 'by', 'technology', 'making', 'us', 'question', 'that', 'will', 'ai', 'even', 'edge', 'us', 'out', 'of', 'our', 'brain', 'jobs', 'too', 'now', 'that', 'can', 'be', 'terrifying', 'however', 'as', 'much', 'as', 'machines', 'will', 'be', 'replacing', 'few', 'jobs', 'they’ll', 'also', 'be', 'creating', 'new', 'jobs', 'with', 'the', 'economic', 'growth', 'innovation', 'and', 'investment', 'around', 'million', 'jobs', 'are', 'said', 'to', 'be', 'generated', 'these', 'newly', 'enhanced', 'jobs', 'are', 'to', 'create', 'benefits', 'and', 'amplify', 'one’s', 'creativity', 'strategy', 'and', 'entrepreneurial', 'skills', 'so', 'what', 'is', 'the', 'catch', 'well', 'it’s', 'the', 'skills', 'even', 'though', 'ai', 'is', 'creating', 'times', 'more', 'jobs', 'than', 'it', 'is', 'destroying', 'it’s', 'the', 'skills', 'that', 'count', 'ai', 'surged', 'in', 'new', 'job', 'opportunities', 'opportunities', 'like', 'senior', 'data', 'scientist', 'mobile', 'application', 'developer', 'and', 'seo', 'specialist', 'these', 'jobs', 'were', 'once', 'never', 'heard', 'of', 'but', 'now', 'with', 'ai', 'it’s', 'born', 'however', 'to', 'do', 'these', 'jobs', 'or', 'for', 'its', 'qualification', 'one', 'needs', 'highlevel', 'skills', 'and', 'to', 'acquire', 'those', 'skills', 'can', 'be', 'an', 'expensive', 'and', 'timeconsuming', 'task', 'the', 'future', 'generation', 'might', 'be', 'able', 'to', 'cope', 'up', 'with', 'it', 'but', 'the', 'real', 'struggle', 'is', 'to', 'be', 'faced', 'by', 'the', 'present', 'two', 'generations', 'it’s', 'the', 'vulnerability', 'between', 'the', 'skill', 'gap', 'and', 'unemployment', 'and', 'the', 'youths', 'are', 'the', 'ones', 'to', 'be', 'crushed', 'the', 'most', 'therefore', 'as', 'the', 'advancement', 'of', 'ai', 'becomes', 'inevitable', 'there', 'remains', 'no', 'choice', 'but', 'to', 'adapt', 'learn', 'equip', 'ourselves', 'and', 'grow', 'with', 'it', 'the', 'companies', 'have', 'to', 'work', 'together', 'to', 'build', 'an', 'aiready', 'workplace', 'they', 'should', 'collaborate', 'with', 'the', 'government', 'educators', 'and', 'nonprofit', 'organizations', 'and', 'work', 'together', 'to', 'bring', 'out', 'policies', 'that', 'could', 'help', 'understand', 'the', 'technologies’', 'impacts', 'faster', 'while', 'also', 'providing', 'the', 'employees', 'some', 'security', 'the', 'economic', 'and', 'business', 'planning', 'should', 'be', 'made', 'considerable', 'for', 'minimizing', 'the', 'impact', 'on', 'local', 'jobs', 'and', 'properly', 'maximizing', 'the', 'opportunities', 'the', 'employees', 'should', 'be', 'provided', 'with', 'proper', 'tools', 'to', 'carry', 'along', 'with', 'the', 'new', 'opportunities', 'while', 'acquiring', 'aibased', 'skills', 'for', 'their', 'daytoday', 'work', 'new', 'skills', 'should', 'be', 'identified', 'and', 'implemented', 'for', 'the', 'upskilling', 'and', 'continual', 'learning', 'initiatives', 'employees', 'will', 'have', 'to', 'maximize', 'their', 'robotic', 'quotient', 'and', 'learn', 'core', 'skills', 'they’ll', 'have', 'to', 'adapt', 'to', 'new', 'working', 'models', 'and', 'understand', 'their', 'roles', 'in', 'the', 'coming', 'future', 'howsoever', 'it’s', 'not', 'like', 'ai', 'will', 'totally', 'take', 'over', 'control', 'even', 'though', 'ai', 'proves', 'to', 'be', 'a', 'better', 'choice', 'it', 'still', 'has', 'its', 'limitations', 'at', 'present', 'first', 'it’s', 'expensive', 'secondly', 'manufacturing', 'machines', 'in', 'bulk', 'is', 'not', 'good', 'for', 'the', 'environment', 'machines', 'are', 'also', 'very', 'high', 'maintenance', 'therefore', 'human', 'labor', 'will', 'often', 'come', 'cheaper', 'and', 'so', 'will', 'be', 'considered', 'over', 'machines', 'underdeveloped', 'countries', 'will', 'find', 'it', 'hard', 'to', 'equip', 'their', 'people', 'with', 'the', 'upskilling', 'and', 'reskilling', 'required', 'for', 'ai', 'workplace', 'and', 'so', 'for', 'ai', 'to', 'play', 'a', 'role', 'in', 'those', 'countries', 'might', 'take', 'years', 'ai', 'can', 'also', 'be', 'risky', 'and', 'unethical', 'as', 'it’s', 'hard', 'to', 'figure', 'out', 'who', 'to', 'be', 'held', 'responsible', 'for', 'in', 'cases', 'where', 'an', 'ai', 'went', 'wrong', 'no', 'matter', 'how', 'advanced', 'ai', 'gets', 'there', 'are', 'some', 'skills', 'where', 'humans', 'will', 'always', 'have', 'an', 'upper', 'hand', 'ie', 'soft', 'skills', 'skills', 'like', 'teamwork', 'communication', 'creativity', 'and', 'critical', 'thinking', 'are', 'something', 'that', 'ai', 'hasn’t', 'been', 'able', 'to', 'beat', 'us', 'up', 'to', 'yet', 'and', 'so', 'the', 'value', 'of', 'creativity', 'leadership', 'and', 'emotional', 'intelligence', 'has', 'increased', 'although', 'with', 'machines', 'coming', 'in', 'between', 'humans', 'causing', 'the', 'lack', 'of', 'humantohuman', 'interaction', 'the', 'humans', 'seem', 'to', 'fade', 'away', 'a', 'little', 'with', 'this', 'era', 'comes', 'the', 'need', 'for', 'good', 'leaders', 'leaders', 'who', 'are', 'capable', 'of', 'handling', 'both', 'machines', 'and', 'humans', 'together', 'the', 'ones', 'who', 'are', 'organized', 'enough', 'to', 'manage', 'the', 'skilled', 'and', 'the', 'unskilled', 'employees', 'while', 'providing', 'the', 'unskilled', 'trainees', 'with', 'proper', 'training', 'leaders', 'who', 'hold', 'profound', 'soft', 'skills', 'and', 'encourage', 'teamwork', 'while', 'working', 'along', 'with', 'machines', 'the', 'ones', 'who', 'are', 'patient', 'calm', 'and', 'optimized', 'in', 'conclusion', 'yes', 'ai', 'and', 'machines', 'are', 'going', 'to', 'be', 'very', 'challenging', 'but', 'there’s', 'nothing', 'humans', 'haven’t', 'overcome', 'adaptation', 'and', 'upgradation', 'are', 'going', 'to', 'be', 'the', 'primary', 'factor', 'for', 'survival', 'as', 'we', 'witness', 'the', 'onset', 'of', 'the', 'th', 'industrial', 'revolution', 'let’s', 'buckle', 'up', 'our', 'seats', 'and', 'race', 'along', 'the', 'highway', 'with', 'the', 'essential', 'fuels', 'skills', 'so', 'as', 'to', 'not', 'let', 'ourselves', 'eliminated', 'after', 'all', 'this', 'is', 'an', 'unending', 'race', 'with', 'infinity', 'as', 'the', 'end', 'all', 'we', 'could', 'do', 'is', 'try', 'not', 'to', 'run', 'out', 'of', 'fuel', 'try', 'not', 'to', 'be', 'outdated', 'blackcoffer', 'insights', 'glady', 'karunya', 'institute', 'of', 'technology', 'and', 'sciences', 'tags', 'ai', 'challenges', 'human', 'error', 'human', 'intelligence', 'ml', 'facebook', 'twitter', 'pinterest', 'whatsapp', 'previous', 'article', 'what', 'jobs', 'will', 'robots', 'take', 'from', 'humans', 'in', 'the', 'future', 'next', 'article', 'ai', 'in', 'healthcare', 'to', 'improve', 'patient', 'outcomes', 'ajay', 'bidyarthy', 'related', 'articles', 'more', 'from', 'author', 'rise', 'of', 'telemedicine', 'and', 'its', 'impact', 'on', 'livelihood', 'by', 'the', 'year', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'advertisement', 'most', 'popular', 'insights', 'ppt', 'solution', 'to', 'quadratic', 'assignment', 'problems', 'qap', 'using', 'ant', 'colony', 'system', 'february', 'how', 'is', 'login', 'logout', 'time', 'tracking', 'for', 'employees', 'in', 'office', 'done', 'september', 'how', 'telehealth', 'and', 'telemedicine', 'helping', 'people', 'to', 'fight', 'against', 'covid', 'april', 'iot', 'ai', 'ml', 'to', 'detect', 'fire', 'and', 'smoke', 'june', 'load', 'more', 'recommended', 'insights', 'coronavirus', 'impact', 'on', 'energy', 'markets', 'immigration', 'datawarehouse', 'aibased', 'recommendations', 'impress', 'with', 'a', 'modern', 'website', 'iot', 'aiml', 'descriptive', 'solution', 'outline', 'latest', 'article', 'rise', 'of', 'telemedicine', 'and', 'its', 'impact', 'on', 'livelihood', 'by', 'the', 'year', 'january', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'popular', 'insights', 'rise', 'of', 'telemedicine', 'and', 'its', 'impact', 'on', 'livelihood', 'by', 'the', 'year', 'january', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'rise', 'of', 'ehealth', 'and', 'its', 'impact', 'on', 'humans', 'by', 'the', 'year', 'january', 'popular', 'insights', 'category', 'what', 'we', 'think', 'our', 'success', 'stories', 'blackcoffer', 'healthcare', 'artificial', 'intelligence', 'it', 'big', 'data', 'lifestyle', 'ecommerce', 'online', 'market', 'place', 'about', 'us', 'we', 'provide', 'intelligence', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'datadatadriven', 'dashboards', 'applications', 'development', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique', 'specialist', 'services', 'and', 'highlvel', 'human', 'expertise', 'contact', 'us', 'helloblackcoffercom', 'follow', 'us', 'facebook', 'linkedin', 'twitter', 'youtube', '©', 'all', 'right', 'reserved', 'blackcofferopc', 'pvt', 'ltd', 'our', 'success', 'stories', 'what', 'we', 'do', 'what', 'we', 'think', 'how', 'to', 'schedule', 'demo', 'contact']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for word in text.split()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c4bae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6c0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()    INI UDAH DI DOWNLOAD!!!\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb9c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = 'stopwords.txt'\n",
    "\n",
    "def remove_stopwords(clean_tokens, stopword_file):\n",
    "    with open(stopword_file, 'r') as f:\n",
    "#         stop_words = f.read().splitlines()\n",
    "        stop_words = [word.lower() for word in f.read().splitlines()]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in clean_tokens:\n",
    "        if token.lower() not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b678a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = remove_stopwords(clean_tokens, 'stopwords.txt')\n",
    "clean_tokens = filtered_tokens\n",
    "\n",
    "# print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e499be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4099410",
   "metadata": {},
   "source": [
    "# Positive and Negative scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e79b459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this positive words contain: 95\n",
      "['success', 'diversified', 'intelligence', 'intelligence', 'recovery', 'recover', 'recovery', 'recover', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'recommendation', 'diversified', 'support', 'intelligence', 'intelligence', 'fascination', 'intelligence', 'wonders', 'admiration', 'captivating', 'pure', 'intelligence', 'supreme', 'intelligence', 'supremacy', 'intelligence', 'work', 'intelligence', 'work', 'work', 'surpass', 'greatest', 'defeated', 'top', 'skilled', 'defeated', 'supremacy', 'helped', 'intelligence', 'skilled', 'work', 'perfection', 'improves', 'work', 'led', 'leading', 'lead', 'surpass', 'intelligence', 'innovation', 'enhanced', 'benefits', 'skill', 'work', 'work', 'faster', 'properly', 'proper', 'work', 'proves', 'cheaper', 'advanced', 'soft', 'intelligence', 'capable', 'skilled', 'proper', 'profound', 'soft', 'encourage', 'patient', 'calm', 'survival', 'intelligence', 'improve', 'patient', 'popular', 'helping', 'recommended', 'recommendations', 'impress', 'modern', 'popular', 'popular', 'success', 'intelligence', 'intelligence', 'innovation', 'extraordinary', 'success']\n"
     ]
    }
   ],
   "source": [
    "# enter = input()  # clean_tokens, tokens, full_text\n",
    "\n",
    "# membuka file positive words dan menyimpannya dalam set\n",
    "with open('positive-words.txt', 'r') as f:\n",
    "    positive_words = set(f.read().split())\n",
    "\n",
    "# membagi teks menjadi daftar kata\n",
    "words = clean_tokens\n",
    "\n",
    "# mencocokkan setiap kata dalam teks dengan set kata positif\n",
    "positive = [word for word in words if word.lower() in positive_words]\n",
    "\n",
    "# mencetak kata-kata positif yang ditemukan\n",
    "print('this positive words contain:', len(positive))\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33c6b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this positive words contain: 45\n",
      "['tanks', 'tanks', 'tanks', 'tanks', 'problems', 'dust', 'appalled', 'unravel', 'limitations', 'greed', 'havoc', 'lost', 'unskilled', 'fear', 'fear', 'threat', 'oppose', 'threat', 'lose', 'fearful', 'scary', 'drowning', 'challenging', 'risk', 'lost', 'expensive', 'struggle', 'crushed', 'inevitable', 'limitations', 'expensive', 'hard', 'risky', 'unethical', 'hard', 'wrong', 'critical', 'lack', 'unskilled', 'unskilled', 'challenging', 'buckle', 'error', 'problems', 'smoke']\n"
     ]
    }
   ],
   "source": [
    "# membuka file positive words dan menyimpannya dalam set\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    negative_words = set(f.read().split())\n",
    "\n",
    "# membagi teks menjadi daftar kata\n",
    "words = clean_tokens\n",
    "\n",
    "# mencocokkan setiap kata dalam teks dengan set kata positif\n",
    "negative = [word for word in words if word.lower() in negative_words]\n",
    "\n",
    "# mencetak kata-kata positif yang ditemukan\n",
    "print('this positive words contain:', len(negative))\n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49002d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive score: 95\n",
      "negative score: -45\n",
      "total score (positive - negative): 50\n"
     ]
    }
   ],
   "source": [
    "num_positive = len(positive) * 1\n",
    "num_negative = len(negative) * -1\n",
    "total_score = num_positive + num_negative\n",
    "\n",
    "print('positive score:', num_positive)\n",
    "print('negative score:', num_negative)\n",
    "print('total score (positive - negative):', total_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58beb732",
   "metadata": {},
   "source": [
    "# Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81dc2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity score adalah: 0.3571\n",
      "positive score: 95\n",
      "negative score: 45\n"
     ]
    }
   ],
   "source": [
    "positive_score = len(positive) * 1\n",
    "negative_score = len(negative) * 1\n",
    "\n",
    "# polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.00001)\n",
    "polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.00001)\n",
    "\n",
    "# Range is from -1 to +1\n",
    "print('polarity score adalah:', round(polarity_score, 4))\n",
    "print('positive score:', positive_score)\n",
    "print('negative score:', negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9761df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity score with library: 0.09935924332078179\n"
     ]
    }
   ],
   "source": [
    "# print((120 - -39) / (120 + -39 + 0.0001))\n",
    "# !pip install textblob   UDAH DI INSTALL\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)\n",
    "polarity_score = blob.sentiment.polarity\n",
    "print('Polarity score with library:',polarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df18c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity score: 0.12589928046232077\n"
     ]
    }
   ],
   "source": [
    "# CARI SUBJEKTIVITY SCORE\n",
    "totalWords_afterCleaning = len(clean_tokens)\n",
    "\n",
    "subjectivity_score = (positive_score + negative_score)/ ((totalWords_afterCleaning) + 0.000001)\n",
    "# Range is from 0 to +1\n",
    "print('Subjectivity score:', subjectivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec1779",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "251e193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "\n",
    "def is_complex(word):\n",
    "    \"\"\"\n",
    "    Checks if a given word is complex or not.\n",
    "    A word is considered complex if it has three or more syllables and is not a proper noun.\n",
    "    \"\"\"\n",
    "    if len(word) < 3 or word[0].isupper():\n",
    "        return False\n",
    "    syllable_count = syllables.estimate(word)\n",
    "    return syllable_count >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82d1ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentence length is 30.16 words.\n"
     ]
    }
   ],
   "source": [
    "def calculate_avg_sentence_length(full_text):\n",
    "    # Split the text into sentences\n",
    "    sentences = full_text.split('.')\n",
    "    \n",
    "    # Count the number of sentences and words\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(full_text.split())\n",
    "    \n",
    "    # Calculate the average sentence length\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    \n",
    "    # Return the average sentence length\n",
    "    return avg_sentence_length\n",
    "\n",
    "# text = \"This is a sample sentence. It has more than one sentence in it. The end.\"\n",
    "avg_sentence_length = calculate_avg_sentence_length(full_text)\n",
    "# print(f\"The average sentence length is {avg_sentence_length:.2f} words.\")\n",
    "print(f\"The average sentence length is {round(avg_sentence_length,2)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bb3a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of sentences: 79\n",
      "num of words: 2383\n"
     ]
    }
   ],
   "source": [
    "sentences = full_text.split('.')\n",
    "num_sentences = len(sentences)\n",
    "num_words = len(full_text.split())    \n",
    "\n",
    "print('num of sentences:', num_sentences)\n",
    "print('num of words:', num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "365f7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n"
     ]
    }
   ],
   "source": [
    "num_complex_words = 0\n",
    "for word in words:\n",
    "    if is_complex(word):\n",
    "        num_complex_words += 1\n",
    "percent_complex_words = (num_complex_words / num_words) #* 100\n",
    "print(num_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6979056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fog index: 12.15159701896875\n",
      "percentage of complex words: 0.21443558539655896\n"
     ]
    }
   ],
   "source": [
    "fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "print('fog index:', fog_index)\n",
    "print('percentage of complex words:', percent_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20acb026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of sentences: 79\n",
      "num of words: 2383\n"
     ]
    }
   ],
   "source": [
    "sentences = full_text.split('.')\n",
    "num_sentences = len(sentences)\n",
    "num_words = len(full_text.split())    \n",
    "\n",
    "print('num of sentences:', num_sentences)\n",
    "print('num of words:', num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49842739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 28.688172043010752\n",
      "Percentage of complex words: 0.21326836581709147\n",
      "Fog Index: 11.560576163531138\n"
     ]
    }
   ],
   "source": [
    "import syllables\n",
    "\n",
    "def is_complex(word):\n",
    "    \"\"\"\n",
    "    Checks if a given word is complex or not.\n",
    "    A word is considered complex if it has three or more syllables and is not a proper noun.\n",
    "    \"\"\"\n",
    "    if len(word) < 3 or word[0].isupper():\n",
    "        return False\n",
    "    syllable_count = syllables.estimate(word)\n",
    "    return syllable_count >= 3\n",
    "\n",
    "def analyze_readability(full_text):\n",
    "    \"\"\"\n",
    "    Analyzes the readability of a given text.\n",
    "    Returns a tuple of (average sentence length, percentage of complex words, Fog Index).\n",
    "    \"\"\"\n",
    "    # Calculate average sentence length\n",
    "    sentences = nltk.sent_tokenize(full_text)\n",
    "    num_sentences = len(sentences)\n",
    "    words = nltk.word_tokenize(full_text)\n",
    "    num_words = len(words)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "    # Calculate percentage of complex words\n",
    "    num_complex_words = 0\n",
    "    for word in words:\n",
    "        if is_complex(word):\n",
    "            num_complex_words += 1\n",
    "    percent_complex_words = (num_complex_words / num_words) #* 100\n",
    "\n",
    "    # Calculate Fog Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "\n",
    "    return (avg_sentence_length, percent_complex_words, fog_index)\n",
    "\n",
    "# text = \"The quick brown fox jumps over the lazy dog. This sentence has some complex words like 'jumps' and 'sentence'.\"\n",
    "result = analyze_readability(full_text)\n",
    "print(\"Average sentence length:\", result[0])\n",
    "print(\"Percentage of complex words:\", result[1])\n",
    "print(\"Fog Index:\", result[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8e9e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "2668\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(full_text)\n",
    "num_sentences = len(sentences)\n",
    "words = nltk.word_tokenize(full_text)\n",
    "num_words = len(words)\n",
    "print(num_sentences)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a56daa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n"
     ]
    }
   ],
   "source": [
    "num_complex_words = 0\n",
    "for word in words:\n",
    "    if is_complex(word):\n",
    "        num_complex_words += 1\n",
    "# percent_complex_words = (num_complex_words / num_words) #* 100\n",
    "print(num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed61634",
   "metadata": {},
   "source": [
    "# mencari Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b6c972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Words Per Sentence: 28.688172043010752\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(full_text)\n",
    "num_sentences = len(sentences)\n",
    "count_words_per_sentence = [len(nltk.word_tokenize(sentence)) for sentence in sentences] \n",
    "total_num_words = sum(count_words_per_sentence)\n",
    "\n",
    "avg_words_per_sentence = total_num_words / num_sentences\n",
    "print('Average Number of Words Per Sentence:', avg_words_per_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2201054",
   "metadata": {},
   "source": [
    "# WORD COUNT (cleaned words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6182db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Total cleaned words present in the text: 1112\n"
     ]
    }
   ],
   "source": [
    "num_word_count = len(clean_tokens)\n",
    "print('the Total cleaned words present in the text:',num_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dde5a",
   "metadata": {},
   "source": [
    "# Avg Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d3672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rata-rata panjang kata: 5.262694083088544\n"
     ]
    }
   ],
   "source": [
    "# s = 'saya makan nasi di dapur'\n",
    "\n",
    "# Menggunakan method split() untuk memisahkan kata dalam string\n",
    "words = full_text.split()\n",
    "\n",
    "# Menghitung total jumlah karakter di setiap kata\n",
    "total_char_count = sum(len(word) for word in words)\n",
    "\n",
    "# Menghitung jumlah kata dalam string\n",
    "num_words = len(words)\n",
    "\n",
    "# Menghitung rata-rata panjang kata\n",
    "avg_word_length = total_char_count / num_words\n",
    "\n",
    "# Mencetak hasil\n",
    "print(\"Rata-rata panjang kata:\", avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d27e10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14980"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9fc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
